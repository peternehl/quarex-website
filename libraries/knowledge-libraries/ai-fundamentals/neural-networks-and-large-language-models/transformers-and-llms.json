{
  "name": "Transformers and LLMs",
  "chapters": [
    {
      "name": "From Word Embeddings to Transformers",
      "topics": [],
      "tags": [
        "technology",
        "innovation",
        "ai-ml",
        "computer-science"
      ]
    },
    {
      "name": "Attention Mechanisms and Context Windows",
      "topics": [],
      "tags": [
        "technology",
        "ai-ml",
        "innovation",
        "education"
      ]
    },
    {
      "name": "How Large Language Models Are Trained",
      "topics": [],
      "tags": [
        "technology",
        "ai-ml",
        "innovation",
        "neural-networks"
      ]
    },
    {
      "name": "Why LLMs Predict Text Rather Than 'Think'",
      "topics": [],
      "tags": [
        "technology",
        "ai-ml",
        "critical-thinking",
        "epistemology"
      ]
    }
  ]
}